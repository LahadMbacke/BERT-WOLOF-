{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f370503",
   "metadata": {},
   "source": [
    "# Data preparation for BERT pretraining (Wolof)\n",
    "\n",
    "This notebook prepares a DataLoader for BERT pretraining (MLM + NSP) from a CSV file containing a text column.\n",
    "Each step is separated into its own cell: data loading, pair creation for NSP, tokenization, masking for MLM, dataset and DataLoader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc760cdb",
   "metadata": {},
   "source": [
    "## 1) Imports and configuration\n",
    "\n",
    "Import dependencies and set global constants (e.g. CSV path, sequence length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4208c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Configuration (change as needed)\n",
    "CSV_PATH = 'wolof_data.csv'  # path to your CSV\n",
    "TEXT_COL = 'texte_wolof'    # column name that contains text\n",
    "PRE_TRAIN_SEQ_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "MASKED_LM_PROB = 0.15\n",
    "MAX_PREDICTIONS_PER_SEQ = 20\n",
    "TOKENIZER_NAME = 'bert-base-uncased'  # replace if you have a custom tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e7158c",
   "metadata": {},
   "source": [
    "## 2) Load CSV and preview\n",
    "\n",
    "Read the CSV file and inspect a quick preview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6328f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV (modify path if needed)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print('Total rows in CSV:', len(df))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654bba3",
   "metadata": {},
   "source": [
    "## 3) Prepare sentence list\n",
    "\n",
    "Extract the text column and clean (drop empty rows). Each line is treated as a candidate sentence here; split further if your rows contain multiple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d7ce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[TEXT_COL].dropna().astype(str).tolist()\n",
    "sentences = [t.strip() for t in texts if t.strip()]\n",
    "print(f'Prepared {len(sentences)} candidate sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406a29f1",
   "metadata": {},
   "source": [
    "## 4) Create pairs for NSP (Next Sentence Prediction)\n",
    "\n",
    "Create pairs (A, B) where B is often the true next sentence (is_next=1) and sometimes a random sentence (is_next=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3393c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_pairs(sentences, dup_ratio=0.5):\n",
    "    pairs = []\n",
    "    for i in range(len(sentences)-1):\n",
    "        if random.random() < dup_ratio:\n",
    "            a = sentences[i]\n",
    "            b = sentences[i+1]\n",
    "            is_next = 1\n",
    "        else:\n",
    "            a = sentences[i]\n",
    "            b = random.choice(sentences)\n",
    "            is_next = 0\n",
    "        pairs.append((a, b, is_next))\n",
    "    return pairs\n",
    "\n",
    "pairs = create_sentence_pairs(sentences)\n",
    "print(f'Created {len(pairs)} sentence pairs')\n",
    "print('Examples:', pairs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6b8a11",
   "metadata": {},
   "source": [
    "## 5) Tokenizer and parameters\n",
    "\n",
    "Initialize the BERT tokenizer (or your trained tokenizer) and set special token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b955d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(TOKENIZER_NAME, use_fast=True)\n",
    "pad_id = tokenizer.pad_token_id or 0\n",
    "mask_id = tokenizer.mask_token_id\n",
    "cls_id = tokenizer.cls_token_id\n",
    "sep_id = tokenizer.sep_token_id\n",
    "print('Vocab size:', tokenizer.vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae5085",
   "metadata": {},
   "source": [
    "## 6) Encoding pairs (helper)\n",
    "\n",
    "Tokenize A and B without special tokens then build [CLS] A [SEP] B [SEP] and the segment ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555483d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pair(tokenizer, a, b, max_len):\n",
    "    a_ids = tokenizer.encode(a, add_special_tokens=False)\n",
    "    b_ids = tokenizer.encode(b, add_special_tokens=False)\n",
    "    max_total = max_len - 3  # reserve [CLS],[SEP],[SEP]\n",
    "    while len(a_ids) + len(b_ids) > max_total:\n",
    "        if len(a_ids) > len(b_ids):\n",
    "            a_ids.pop()\n",
    "        else:\n",
    "            b_ids.pop()\n",
    "    input_ids = [cls_id] + a_ids + [sep_id] + b_ids + [sep_id]\n",
    "    seg_zero_len = 1 + len(a_ids) + 1\n",
    "    seg_ids = [0 if i < seg_zero_len else 1 for i in range(len(input_ids))]\n",
    "    return input_ids, seg_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07782051",
   "metadata": {},
   "source": [
    "## 7) Masking for MLM (BERT style)\n",
    "\n",
    "Create `masked_lm_labels` and apply the 80/10/10 rule (mask/random/original). Here we use 0 as the `ignore_index` value (you may change this to -100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b628ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_lm_labels(input_ids, tokenizer, masked_lm_prob=0.15, max_predictions_per_seq=20):\n",
    "    cand_indexes = []\n",
    "    special_ids = {cls_id, sep_id, pad_id}\n",
    "    for i, token in enumerate(input_ids):\n",
    "        if token in special_ids:\n",
    "            continue\n",
    "        cand_indexes.append(i)\n",
    "    num_to_mask = min(max_predictions_per_seq, max(1, int(round(len(input_ids) * masked_lm_prob))))\n",
    "    masked_positions = random.sample(cand_indexes, num_to_mask) if len(cand_indexes) >= num_to_mask else cand_indexes\n",
    "    labels = [0] * len(input_ids)  # 0 = ignore (trainer uses ignore_index=0)\n",
    "    for pos in masked_positions:\n",
    "        orig = input_ids[pos]\n",
    "        prob = random.random()\n",
    "        if prob < 0.8:\n",
    "            input_ids[pos] = mask_id\n",
    "        elif prob < 0.9:\n",
    "            input_ids[pos] = random.randrange(tokenizer.vocab_size)\n",
    "        else:\n",
    "            input_ids[pos] = orig\n",
    "        labels[pos] = orig\n",
    "    return input_ids, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb485505",
   "metadata": {},
   "source": [
    "## 8) PyTorch Dataset for pretraining\n",
    "\n",
    "Dataset class that returns a dict with `input_ids`, `segment_labels`, `is_next`, `masked_lm_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b706c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPretrainingDataset(Dataset):\n",
    "    def __init__(self, pairs, tokenizer, max_len=128, masked_lm_prob=0.15, max_predictions=20):\n",
    "        self.pairs = pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.masked_lm_prob = masked_lm_prob\n",
    "        self.max_predictions = max_predictions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a, b, is_next = self.pairs[idx]\n",
    "        input_ids, seg_ids = encode_pair(self.tokenizer, a, b, self.max_len)\n",
    "        input_ids_masked, mlm_labels = create_masked_lm_labels(input_ids.copy(), self.tokenizer, self.masked_lm_prob, self.max_predictions)\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids_masked, dtype=torch.long),\n",
    "            'segment_labels': torch.tensor(seg_ids, dtype=torch.long),\n",
    "            'is_next': torch.tensor(is_next, dtype=torch.long),\n",
    "            'masked_lm_labels': torch.tensor(mlm_labels, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c144e3",
   "metadata": {},
   "source": [
    "## 9) Collate function (padding)\n",
    "\n",
    "Custom collate function to pad tensors within the batch. We use 0 as the `ignore` value for `masked_lm_labels` (adjust if you change `ignore_index`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2976d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def collate_fn(batch: List[dict]):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    segs = [item['segment_labels'] for item in batch]\n",
    "    is_next = torch.stack([item['is_next'] for item in batch])\n",
    "    mlm_labels = [item['masked_lm_labels'] for item in batch]\n",
    "    padded_input = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=pad_id)\n",
    "    padded_segs = torch.nn.utils.rnn.pad_sequence(segs, batch_first=True, padding_value=0)\n",
    "    padded_mlm_labels = torch.nn.utils.rnn.pad_sequence(mlm_labels, batch_first=True, padding_value=0)\n",
    "    return {\n",
    "        'input_ids': padded_input,\n",
    "        'segment_labels': padded_segs,\n",
    "        'is_next': is_next,\n",
    "        'masked_lm_labels': padded_mlm_labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce70f90",
   "metadata": {},
   "source": [
    "## 10) Build Dataset and DataLoader\n",
    "\n",
    "Create the PyTorch dataset, DataLoader and test one batch to verify shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39286652",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BertPretrainingDataset(pairs, tokenizer, max_len=PRE_TRAIN_SEQ_LEN,\n",
    "                                 masked_lm_prob=MASKED_LM_PROB, max_predictions=MAX_PREDICTIONS_PER_SEQ)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Test one batch\n",
    "batch = next(iter(dataloader))\n",
    "print({k: v.shape for k, v in batch.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b497bd",
   "metadata": {},
   "source": [
    "## 11) Notes / Next steps\n",
    "\n",
    "- If you want to follow the HuggingFace convention, use `-100` as the `ignore_index` value for `masked_lm_labels` and update `bert/train.py` to `nn.CrossEntropyLoss(ignore_index=-100)`.\n",
    "- To train: instantiate a pretraining model (e.g. your repo's BERT pretraining model), move it to `device`, then use the trainer with the created `dataloader`.\n",
    "- For a large corpus: consider serializing tokenized pairs to speed up epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERT_Wolof (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
